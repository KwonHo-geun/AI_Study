{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KwonHo-geun/AI_Study/blob/main/Face_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 얼굴 데이터셋과 랜드마크(landmark)\n",
        "\n",
        "## 1. 얼굴 데이터셋을 다루는 이유\n",
        "- 얼굴 인식 및 분석은 **보안, 감정 분석, HCI(Human-Computer Interaction), AR/VR** 등 다양한 응용 분야에서 활용됨.\n",
        "- 데이터셋은 모델이 얼굴 구조와 패턴을 학습하는 데 핵심적 역할을 함.\n",
        "- 예시:\n",
        "  - 얼굴 검출 (Face Detection)\n",
        "  - 얼굴 정렬 (Face Alignment)\n",
        "  - 표정/감정 분석\n",
        "  - 신원 인식 (Face Recognition)\n",
        "\n",
        "## 2. Landmark란?\n",
        "- **Landmark**는 얼굴의 중요한 특징점(keypoint)을 의미.\n",
        "- 일반적으로 눈, 코, 입, 턱선 등 위치를 점으로 표시.\n",
        "- Landmark는 얼굴 정렬(얼굴 각도 보정), 표정 분석, 마스크 착용 여부 판단 등에서 활용됨.\n",
        "\n",
        "## 3. Landmark의 다양한 토폴로지\n",
        "- **5-point landmark**\n",
        "  - 두 눈, 코 끝, 입 양쪽.\n",
        "  - 간단한 정렬(face alignment)에 자주 사용.\n",
        "- **68-point landmark**\n",
        "  - 얼굴 외곽, 눈썹, 눈, 코, 입 전체를 세밀하게 포함.\n",
        "  - 표정 인식, 세밀한 얼굴 분석에 활용.\n",
        "- **더 확장된 형태**\n",
        "  - 98-point landmark (예: WFLW dataset)\n",
        "  - 194-point landmark (고해상도 얼굴 연구용)\n",
        "  - 특정 연구 목적(예: micro-expression 분석)에 맞추어 더 많은 점을 활용하기도 함.\n",
        "\n",
        "<br>\n",
        "\n",
        "68pts landmark 예시:\n",
        "\n",
        "![68pts landmark 예시](https://www.researchgate.net/profile/Ana_Tanevska/publication/320979643/figure/fig3/AS:636979167383553@1528879076541/The-68-facial-landmarks-extracted-from-a-frontal-face-view.png)\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "## 4. 공개된 Face Dataset 예시\n",
        "- **LFW (Labeled Faces in the Wild)**  \n",
        "  - \"in-the-wild\" 환경에서 촬영된 얼굴 이미지 데이터셋. 얼굴 인식 벤치마크에 많이 쓰임.\n",
        "- **CelebA**  \n",
        "  - 20만 장 이상의 유명인 얼굴 이미지. 속성(attribute) 레이블 포함 (예: 안경 착용 여부, 미소 여부).\n",
        "- **WFLW (Wider Facial Landmarks in-the-Wild)**  \n",
        "  - 98점 landmark 제공. 다양한 표정, 조명, 가림 현상이 포함.\n",
        "- **300W**  \n",
        "  - 여러 landmark 데이터셋을 통합. 얼굴 landmark 검출 연구에서 표준으로 사용됨.\n",
        "- **AffectNet**  \n",
        "  - 감정 레이블이 포함된 대규모 얼굴 데이터셋. 감정 분석에 활용.\n",
        "\n",
        "<br>\n",
        "\n",
        "## 5. 이론과 코드의 연결\n",
        "- 코드에서는 보통 **얼굴 검출(Face Detection)** 후 → **Landmark 추출** → 정렬 및 분석을 수행.\n",
        "- Landmark는 단순히 점 좌표 집합이 아니라, 얼굴을 수학적으로 정렬하고 딥러닝 모델 입력을 정규화하는 데 필수.\n",
        "- 따라서 코드 학습 전, landmark의 역할과 데이터셋 특성을 이해하는 것이 매우 중요함.\n"
      ],
      "metadata": {
        "id": "jx0nMj5Uge2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dimg.donga.com/wps/NEWS/IMAGE/2021/03/26/106095249.2.jpg"
      ],
      "metadata": {
        "id": "ufvfTb5t6uuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenCV ViolaJones"
      ],
      "metadata": {
        "id": "Lfku83-Aerg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://docs.opencv.org/3.4/db/d28/tutorial_cascade_classifier.html"
      ],
      "metadata": {
        "id": "3-w40hqeepIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://github.com/kipr/opencv/tree/master/data/haarcascades\n",
        "!wget https://github.com/kipr/opencv/raw/master/data/haarcascades/haarcascade_frontalface_default.xml"
      ],
      "metadata": {
        "id": "IiWdFfByews9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "face_cascade = cv2.CascadeClassifier()\n",
        "face_cascade.load('haarcascade_frontalface_default.xml')"
      ],
      "metadata": {
        "id": "M7BGQZr1glVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bgr = cv2.imread('/content/106095249.2.jpg')\n",
        "gray = cv2.cvtColor(bgr, cv2.COLOR_BGR2GRAY)"
      ],
      "metadata": {
        "id": "uY0NPl_9S4ry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gray"
      ],
      "metadata": {
        "id": "FZHuTETYS5bE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gray = cv2.equalizeHist(gray)\n",
        "#-- Detect faces\n",
        "faces = face_cascade.detectMultiScale(gray)"
      ],
      "metadata": {
        "id": "3ZE6J0o7VJg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "faces\n"
      ],
      "metadata": {
        "id": "wFOjvzBvVKuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dst = bgr.copy()\n",
        "for (x,y,w,h) in faces:\n",
        "    cv2.rectangle(dst, (x, y), (x+w, y+h), (255, 0, 0), 3)\n",
        "cv2_imshow(dst)"
      ],
      "metadata": {
        "id": "H0p-kXSChI1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 딥러닝 기반"
      ],
      "metadata": {
        "id": "ftMk3TWKeqV1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WIqHYHK6Zv8"
      },
      "outputs": [],
      "source": [
        "!pip install mtcnn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mtcnn"
      ],
      "metadata": {
        "id": "_yFju93i6k-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "detector = mtcnn.MTCNN()"
      ],
      "metadata": {
        "id": "hgtIXANW6qhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://img.hankyung.com/photo/202101/01.25062168.1.jpg -O girls.jpg"
      ],
      "metadata": {
        "id": "PaJ8cYaNfOFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = cv2.imread('girls.jpg')\n",
        "faces = detector.detect_faces(image[..., [2, 1, 0]])"
      ],
      "metadata": {
        "id": "nqz5dUZdfVch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image"
      ],
      "metadata": {
        "id": "qiVldeeFr3Hi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "faces[0]['box']\n",
        "faces[0]['keypoints']\n"
      ],
      "metadata": {
        "id": "zEsxvIlgfhXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dst = image.copy()\n",
        "# for i in range(len(faces)):\n",
        "#     face = faces[i]\n",
        "for face in faces:\n",
        "    x1, y1, w, h = face['box']\n",
        "    x2, y2 = x1 + w, y1 + h\n",
        "    cv2.rectangle(dst, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
        "\n",
        "    for key, value in face['keypoints'].items():\n",
        "        cv2.circle(dst, value, 2, (0, 0, 255), -1)\n",
        "cv2_imshow(dst)\n",
        "\n"
      ],
      "metadata": {
        "id": "8G7epj3Bho2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "image = cv2.imread('/content/106095249.2.jpg')\n",
        "faces = detector.detect_faces(image[..., [2, 1, 0]])\n",
        "\n",
        "dst = image.copy()\n",
        "for face in faces:\n",
        "    print(face)\n",
        "    x1, y1, w, h = face['box']\n",
        "    x2, y2 = x1 + w, y1 + h\n",
        "    cv2.rectangle(dst, (x1, y1), (x2, y2), (0,0,255), 3)\n",
        "    keypoints = face['keypoints']\n",
        "    for part in ['left_eye', 'right_eye', 'nose', 'mouth_left', 'mouth_right']:\n",
        "        point = keypoints[part]\n",
        "        cv2.circle(dst, point, 3, (0, 255, 0), cv2.FILLED)\n",
        "\n",
        "cv2_imshow(dst)"
      ],
      "metadata": {
        "id": "84oxqCRA7TtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dlib\n",
        "!wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2"
      ],
      "metadata": {
        "id": "n4f62oBJ74o_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!bzip2 -d shape_predictor_68_face_landmarks.dat.bz2"
      ],
      "metadata": {
        "id": "VTEc-AP5-Vra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "detector = dlib.get_frontal_face_detector()\n",
        "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")"
      ],
      "metadata": {
        "id": "igQ6YYEr9dHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=zzing0907&logNo=221612308385"
      ],
      "metadata": {
        "id": "PkNZqfETAVz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_circle(part, dst, color):\n",
        "    cv2.circle(dst, (part.x, part.y), 3, color, -1)\n",
        "\n",
        "image = cv2.imread('/content/106095249.2.jpg')\n",
        "dst = image.copy()\n",
        "rects = detector(image[..., [2, 1, 0]].copy(), 1)\n",
        "for rect in rects:\n",
        "    x1, x2, y1, y2 = rect.left(), rect.right(), rect.top(), rect.bottom()\n",
        "    shape = predictor(image[..., [2, 1, 0]].copy(), rect)\n",
        "    for j in range(17):\n",
        "        part = shape.part(j)\n",
        "        cv2.circle(dst, (part.x, part.y), 3, (0, 255, 0), -1)\n",
        "    for j in range(17, 22):\n",
        "        part = shape.part(j)\n",
        "        cv2.circle(dst, (part.x, part.y), 3, (0, 0, 255), -1)\n",
        "    for j in range(22, 27):\n",
        "        part = shape.part(j)\n",
        "        cv2.circle(dst, (part.x, part.y), 3, (255, 0, 0), -1)\n",
        "    for j in range(27, 36):\n",
        "        part = shape.part(j)\n",
        "        cv2.circle(dst, (part.x, part.y), 3, (255, 255, 0), -1)\n",
        "    for j in range(36, 42):\n",
        "        part = shape.part(j)\n",
        "        cv2.circle(dst, (part.x, part.y), 3, (0, 255, 255), -1)\n",
        "    for j in range(42, 48):\n",
        "        part = shape.part(j)\n",
        "        cv2.circle(dst, (part.x, part.y), 3, (255, 0, 255), -1)\n",
        "    for j in range(48, 68):\n",
        "        part = shape.part(j)\n",
        "        cv2.circle(dst, (part.x, part.y), 3, (255, 255, 255), -1)\n",
        "    cv2.rectangle(dst, (x1, y1), (x2, y2), (0, 0, 255), 3)\n",
        "\n",
        "    # index = [36, 45, 57]\n",
        "    # for i in range(3):\n",
        "    #     index1 = index[i]\n",
        "    #     index2 = index[(i + 1) % 3]\n",
        "\n",
        "    index = [(36, 45), (45, 57), (57, 36)]\n",
        "    for index1, index2 in index:\n",
        "        p1 = shape.part(index1)\n",
        "        p2 = shape.part(index2)\n",
        "        cv2.line(dst, (p1.x, p1.y), (p2.x, p2.y), (0, 0, 255), 3)\n",
        "\n",
        "\n",
        "cv2_imshow(dst)"
      ],
      "metadata": {
        "id": "1W71ZrQn-mzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/ageitgey/face_recognition\n",
        "!pip install face_recognition"
      ],
      "metadata": {
        "id": "djZ892DzCah0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import face_recognition"
      ],
      "metadata": {
        "id": "0vXLEjvbESA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_points(dst, points, color):\n",
        "    for point in points:\n",
        "        cv2.circle(dst, point, 3, color, -1)\n",
        "\n",
        "image = cv2.imread('/content/106095249.2.jpg')\n",
        "# image = face_recognition.load_image_file(\"/content/106095249.2.jpg\")\n",
        "face_locations = face_recognition.face_locations(image[..., [2, 1, 0]])\n",
        "face_landmarks_list = face_recognition.face_landmarks(image[..., [2, 1, 0]])\n",
        "dst = image.copy()\n",
        "for x1, y1, x2, y2 in face_locations:\n",
        "    cv2.rectangle(dst, (x1, y1), (x2, y2), (0,0,255), 3)\n",
        "\n",
        "colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), (0, 255, 255), (255, 0, 255), (127, 127, 0), (0, 127, 127), (127, 0, 127)]\n",
        "for landmarks in face_landmarks_list:\n",
        "    for key, color in zip(landmarks.keys(), colors):\n",
        "        draw_points(dst, landmarks[key], color)\n",
        "\n",
        "    print(landmarks)\n",
        "\n",
        "cv2_imshow(dst)"
      ],
      "metadata": {
        "id": "NNWhOFjwEh66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "face_landmarks_list"
      ],
      "metadata": {
        "id": "dVtJOclQJWzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eYJkDl9xd_FQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 얼굴 처리 파이프라인\n",
        "\n",
        "## 1) 얼굴 처리 파이프라인의 전체 흐름\n",
        "- **Detection(검출)**: 이미지/영상에서 얼굴 위치를 찾음 → 바운딩 박스(x, y, w, h 또는 x1, y1, x2, y2).\n",
        "- **Landmark(특징점 추정)**: 박스 내부에서 눈·코·입·윤곽 등 기준점 좌표(2D)를 추정.\n",
        "- **Alignment(정렬)**: 랜드마크 기반으로 회전·기울기·스케일을 보정하여 표준 자세/크기로 맞춤.\n",
        "- **Embedding/Recognition(임베딩·인식)**: 정렬된 얼굴을 벡터(예: 128D)로 변환 → 벡터 간 거리 비교로 동일인 여부 판정.\n",
        "\n",
        "<br>\n",
        "\n",
        "## 2) 검출(Detection) 이론 요약과 실전 유의점\n",
        "### 2.1 전통 방식 (Haar Cascade / Viola–Jones)\n",
        "- **구성**: Haar-like 특징 + AdaBoost 앙상블 + 캐스케이드.\n",
        "- **장점**: 가볍고 빠름, 정면 표준 자세에 적합.\n",
        "- **한계**: 포즈/조명/가림에 취약.\n",
        "\n",
        "### 2.2 딥러닝 기반 (MTCNN, RetinaFace)\n",
        "- **MTCNN**: P-Net → R-Net → O-Net, 박스 + 5점 랜드마크 제공.\n",
        "- **RetinaFace**: 앵커 기반, 박스 + 풍부한 랜드마크 + 품질 정보 제공.\n",
        "\n",
        "### 실무 팁\n",
        "- OpenCV는 BGR, 딥러닝 프레임워크는 RGB → 변환 일관성 유지.\n",
        "- 좌표계 혼용([x, y, w, h] vs [x1, y1, x2, y2]) → 내부 표준화 필요.\n",
        "- 바운딩 박스 경계 넘어가면 클리핑 처리.\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "## 3) 랜드마크(Feature Points)\n",
        "### 정의와 역할\n",
        "- **랜드마크**: 눈꼬리, 콧등, 입꼬리, 턱선 등 해부학적 기준점 2D 좌표.\n",
        "- **주요 용도**:\n",
        "  - 정렬(Alignment) → 임베딩 품질 향상.\n",
        "  - 표정/시선/자세 분석.\n",
        "  - 합성·편집·AR 효과.\n",
        "\n",
        "### 정렬(Alignment)의 수학적 핵심\n",
        "- **유사변환(similarity transform)**: 회전 R + 스케일 s + 평행이동 t.\n",
        "- **최적화 문제**:  \n",
        "![formula](https://latex.codecogs.com/svg.latex?\\min_{s,R,t}\\|sRX+t-Y\\|)  \n",
        "  (프로크루스테스 정렬)\n",
        "- **실무**: 5점(양눈·코·입 좌/우)으로 충분, 정렬 후 112×112 또는 224×224로 리사이즈.\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "## 4) 랜드마크 토폴로지(포인트 세트 설계)\n",
        "- **3점**: 좌·우 눈 + 코끝.\n",
        "- **5점**: 양눈 중심 + 코끝 + 입 좌/우 (가장 널리 쓰임).\n",
        "- **21/27/33점**: 중간 밀도(데이터셋/SDK별).\n",
        "- **68점 (dlib/iBUG 표준)**: 턱 17, 눈썹 10, 코 9, 눈 12, 입 20.\n",
        "- **98점 (WFLW)**: 다양한 포즈/표정/가림 대응.\n",
        "- **106점 (InsightFace)**: 98점 확장.\n",
        "- **194/196점**: 눈꺼풀/입술 안쪽까지 포함.\n",
        "- **468점 (MediaPipe Face Mesh)**: 메시 기반(2D지만 3D 정합 가능).\n",
        "- **Dense Mesh**: 수천~수만 점, 3D Morphable Model 등.\n",
        "\n",
        "### 선택 기준\n",
        "- **정렬/인식 중심** → 5점.\n",
        "- **표정/윤곽 분석** → 68/98/106점.\n",
        "- **AR/메이크업/정밀 편집** → 196/468점.\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "## 5) 얼굴 임베딩/인식\n",
        "- CNN(ResNet, MobileFaceNet 등) → 고정 길이 벡터(128D, 256D).\n",
        "- 학습: 대규모 아이덴티티 분류 + Metric Loss (ArcFace, CosFace).\n",
        "- 추론: 두 벡터 간 거리(코사인/유클리드) 비교 → 동일인 여부 판정.\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "## 6) 데이터셋 실무 처리\n",
        "- **채널**: RGB 기준, OpenCV는 BGR→RGB 변환 필요.\n",
        "- **좌표계**: 내부 표준화 및 변환 유틸 고정.\n",
        "- **정렬 크기**: 112×112, 224×224 등 통일.\n",
        "- **라벨 일관성**: 동일 기준 유지, 어노테이션 검수 필수.\n",
        "- **스플릿 규칙**: 동일 인물은 train/val/test에 동시에 포함 금지.\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "## 7) 평가 지표\n",
        "- **검출**: Precision/Recall, mAP.\n",
        "- **랜드마크**: NME, AUC@0.08, Failure Rate.\n",
        "- **인식**: ROC, EER, TPR@FPR.\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "## 8) 강의용 샘플 이미지\n",
        "- **정면-밝음**: 표준 품질 (정렬 전/후 비교).\n",
        "- **정면-어두움/역광**: 조명 영향 시연.\n",
        "- **측면 ±30°/±60°**: 랜드마크 밀도 차이 비교.\n",
        "- **가림 (마스크/손/안경)**: 실패 사례.\n",
        "- **군중/원거리 다인 이미지**: NMS/중복 검출 사례.\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "## 9) 공개 Face 데이터셋\n",
        "### 검출(Detection)\n",
        "- **WIDER FACE**: 다양한 포즈/규모/가림.\n",
        "- **FDDB**: 전통적 벤치마크.\n",
        "- **MAFA**: 마스크 착용 얼굴.\n",
        "\n",
        "### 랜드마크(Landmark)\n",
        "- **300-W**: 68점 표준.\n",
        "- **WFLW**: 98점 표준, 속성별 평가.\n",
        "- **AFLW**: 광범위 포즈·표정.\n",
        "- **COFW**: 가림 상황 특화.\n",
        "- **Menpo 2D/3D, LaPa**: 2D/3D 및 세밀 랜드마크 포함.\n",
        "\n",
        "### 인식(Recognition)\n",
        "- **LFW**: 전통적 벤치마크.\n",
        "- **VGGFace2**: 대규모, 다양한 포즈·표정.\n",
        "- **CASIA-WebFace**: 학습용 레퍼런스.\n",
        "- **MS1M (MS1MV2)**: 대규모 웹 얼굴 데이터.\n",
        "- **IJB-A/B/C**: 고난도 검증/식별 벤치마크.\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "## 10) 실무 디자인 패턴\n",
        "- **Annotation Assistant**: 박스 기반 자동 랜드마크 추정.\n",
        "- **Detector + Landmark 하이브리드**: YOLO/RetinaFace → 랜드마크 → 인식.\n",
        "- **정렬 후 표준화**: 캐노니컬 좌표 기준 112×112/224×224 고정.\n",
        "- **평가 프로토콜 분리**: Detection/AP, Landmark/NME, Recognition/EER.\n",
        "\n",
        "<br>\n",
        "\n",
        "## 11) SAM 및 생태계 도구 접점\n",
        "- **SAM (Segment Anything)**: 얼굴 윤곽 마스크 자동 추출, 편집/라벨링 보조.\n",
        "- **Hugging Face**: 모델 가중치 및 데모 공유.\n",
        "- **Ultralytics**: YOLO 기반 파이프라인 통합.\n",
        "\n",
        "연계 패턴:  \n",
        "**검출(박스) → 랜드마크 정렬 → 인식/분류 → 필요 시 SAM으로 윤곽 마스크 추가**\n"
      ],
      "metadata": {
        "id": "evXkJNEZiLmA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uFElovIMiZyD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}