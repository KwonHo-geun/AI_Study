{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KwonHo-geun/AI_Study/blob/main/25.07.17_%EC%B0%A8%EB%9F%89%EC%9D%B4%EB%AF%B8%EC%A7%80%EC%98%81%EC%83%81.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAkuWJbCwFH-",
        "outputId": "93b01086-4628-4afb-907d-cd40c02e8c58"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.167-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.2.1)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.7.14)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading ultralytics-8.3.167-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ultralytics-8.3.167 ultralytics-thop-2.0.14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "from ultralytics import YOLO # COCO 사전 훈련된 YOLOv8n 모델 로드\n",
        "\n",
        "model = YOLO(\"yolov8n.pt\") # 모델 정보 표시 (선택사항)\n",
        "model.info() # COCO8 예제 데이터셋으로 100 에포크 훈련\n",
        "results = model.train(data=\"coco8.yaml\", epochs=10, imgsz=640) # 사진 업로드하고 경로 설정\n",
        "#uploaded = files.upload()\n",
        "#video_path = list(uploaded.keys())[0] # 업로드한 이미지에 대해 YOLOv8n 모델로 추론 실행\n",
        "results = model.predict(\n",
        "    source=\"/content/KakaoTalk_20250717_091729282.mp4\",\n",
        "    save=True,\n",
        "    save_txt=True   # ← 이 위치에 배치\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ojHhDhAwFE5",
        "outputId": "fe27e6da-e13c-4a96-9b8a-cd12d289d8d6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YOLOv8n summary: 129 layers, 3,157,200 parameters, 0 gradients, 8.9 GFLOPs\n",
            "Ultralytics 8.3.167 🚀 Python-3.11.13 torch-2.6.0+cu124 CPU (Intel Xeon 2.20GHz)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=coco8.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train3, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/train3, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    897664  ultralytics.nn.modules.head.Detect           [80, [64, 128, 256]]          \n",
            "Model summary: 129 layers, 3,157,200 parameters, 3,157,184 gradients, 8.9 GFLOPs\n",
            "\n",
            "Transferred 355/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 921.7±205.5 MB/s, size: 50.0 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/datasets/coco8/labels/train.cache... 4 images, 0 backgrounds, 0 corrupt: 100%|██████████| 4/4 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 843.3±196.4 MB/s, size: 54.0 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/coco8/labels/val.cache... 4 images, 0 backgrounds, 0 corrupt: 100%|██████████| 4/4 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plotting labels to runs/detect/train3/labels.jpg... \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000119, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train3\u001b[0m\n",
            "Starting training for 10 epochs...\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       1/10         0G      1.082      1.853      1.368         13        640: 100%|██████████| 1/1 [00:04<00:00,  4.25s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.634       0.87      0.888      0.626\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       2/10         0G     0.9278      3.138      1.165         13        640: 100%|██████████| 1/1 [00:03<00:00,  3.68s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:02<00:00,  2.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.703      0.833      0.888      0.613\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       3/10         0G     0.9953      2.825      1.309         13        640: 100%|██████████| 1/1 [00:03<00:00,  3.75s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.725      0.831      0.889      0.611\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       4/10         0G      1.235      2.602      1.454         13        640: 100%|██████████| 1/1 [00:03<00:00,  3.56s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.629      0.833      0.873      0.626\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       5/10         0G     0.8402      2.598      1.306         13        640: 100%|██████████| 1/1 [00:04<00:00,  4.71s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.683      0.833      0.873      0.627\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       6/10         0G      1.049      2.406      1.444         13        640: 100%|██████████| 1/1 [00:03<00:00,  3.53s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.709       0.75      0.879      0.626\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       7/10         0G      1.268       2.48       1.46         13        640: 100%|██████████| 1/1 [00:04<00:00,  4.19s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.767      0.743      0.879      0.619\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       8/10         0G     0.8933      1.874      1.264         13        640: 100%|██████████| 1/1 [00:03<00:00,  3.67s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.741       0.75      0.893      0.625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       9/10         0G     0.9484       1.66      1.182         13        640: 100%|██████████| 1/1 [00:03<00:00,  3.54s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.729       0.75      0.877       0.61\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      10/10         0G     0.7684      1.957       1.22         13        640: 100%|██████████| 1/1 [00:04<00:00,  4.06s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.717       0.75      0.778      0.594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "10 epochs completed in 0.016 hours.\n",
            "Optimizer stripped from runs/detect/train3/weights/last.pt, 6.5MB\n",
            "Optimizer stripped from runs/detect/train3/weights/best.pt, 6.5MB\n",
            "\n",
            "Validating runs/detect/train3/weights/best.pt...\n",
            "Ultralytics 8.3.167 🚀 Python-3.11.13 torch-2.6.0+cu124 CPU (Intel Xeon 2.20GHz)\n",
            "Model summary (fused): 72 layers, 3,151,904 parameters, 0 gradients, 8.7 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.09s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.633      0.871      0.888      0.624\n",
            "                person          3         10      0.696        0.5       0.52      0.279\n",
            "                   dog          1          1      0.398          1      0.995      0.597\n",
            "                 horse          1          2      0.749          1      0.995      0.598\n",
            "              elephant          1          2      0.574      0.723      0.828      0.381\n",
            "              umbrella          1          1      0.566          1      0.995      0.995\n",
            "          potted plant          1          1      0.817          1      0.995      0.895\n",
            "Speed: 2.2ms preprocess, 260.4ms inference, 0.0ms loss, 2.0ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train3\u001b[0m\n",
            "\n",
            "WARNING ⚠️ \n",
            "inference results will accumulate in RAM unless `stream=True` is passed, causing potential out-of-memory\n",
            "errors for large sources or long-running streams and videos. See https://docs.ultralytics.com/modes/predict/ for help.\n",
            "\n",
            "Example:\n",
            "    results = model(source=..., stream=True)  # generator of Results objects\n",
            "    for r in results:\n",
            "        boxes = r.boxes  # Boxes object for bbox outputs\n",
            "        masks = r.masks  # Masks object for segment masks outputs\n",
            "        probs = r.probs  # Class probabilities for classification outputs\n",
            "\n",
            "video 1/1 (frame 1/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 197.4ms\n",
            "video 1/1 (frame 2/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 6 cars, 1 truck, 184.4ms\n",
            "video 1/1 (frame 3/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 194.5ms\n",
            "video 1/1 (frame 4/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 188.5ms\n",
            "video 1/1 (frame 5/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 161.7ms\n",
            "video 1/1 (frame 6/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 117.7ms\n",
            "video 1/1 (frame 7/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 120.6ms\n",
            "video 1/1 (frame 8/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 121.7ms\n",
            "video 1/1 (frame 9/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 117.5ms\n",
            "video 1/1 (frame 10/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 116.7ms\n",
            "video 1/1 (frame 11/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 116.9ms\n",
            "video 1/1 (frame 12/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 135.7ms\n",
            "video 1/1 (frame 13/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 124.3ms\n",
            "video 1/1 (frame 14/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 124.9ms\n",
            "video 1/1 (frame 15/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 125.6ms\n",
            "video 1/1 (frame 16/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 126.4ms\n",
            "video 1/1 (frame 17/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 116.0ms\n",
            "video 1/1 (frame 18/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 118.5ms\n",
            "video 1/1 (frame 19/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 160.2ms\n",
            "video 1/1 (frame 20/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 1 traffic light, 124.4ms\n",
            "video 1/1 (frame 21/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 1 traffic light, 123.1ms\n",
            "video 1/1 (frame 22/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 118.6ms\n",
            "video 1/1 (frame 23/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 1 traffic light, 117.7ms\n",
            "video 1/1 (frame 24/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 1 traffic light, 124.0ms\n",
            "video 1/1 (frame 25/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 145.9ms\n",
            "video 1/1 (frame 26/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 128.1ms\n",
            "video 1/1 (frame 27/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 119.2ms\n",
            "video 1/1 (frame 28/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 118.2ms\n",
            "video 1/1 (frame 29/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 115.7ms\n",
            "video 1/1 (frame 30/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 119.2ms\n",
            "video 1/1 (frame 31/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 118.8ms\n",
            "video 1/1 (frame 32/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 135.8ms\n",
            "video 1/1 (frame 33/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 121.3ms\n",
            "video 1/1 (frame 34/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 123.2ms\n",
            "video 1/1 (frame 35/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 120.9ms\n",
            "video 1/1 (frame 36/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 122.8ms\n",
            "video 1/1 (frame 37/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 122.6ms\n",
            "video 1/1 (frame 38/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 126.0ms\n",
            "video 1/1 (frame 39/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 125.5ms\n",
            "video 1/1 (frame 40/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 124.5ms\n",
            "video 1/1 (frame 41/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 126.8ms\n",
            "video 1/1 (frame 42/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 1 bus, 121.7ms\n",
            "video 1/1 (frame 43/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 1 bus, 123.0ms\n",
            "video 1/1 (frame 44/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 127.8ms\n",
            "video 1/1 (frame 45/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 142.5ms\n",
            "video 1/1 (frame 46/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 133.4ms\n",
            "video 1/1 (frame 47/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 129.5ms\n",
            "video 1/1 (frame 48/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 1 truck, 121.3ms\n",
            "video 1/1 (frame 49/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 122.7ms\n",
            "video 1/1 (frame 50/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 121.7ms\n",
            "video 1/1 (frame 51/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 128.4ms\n",
            "video 1/1 (frame 52/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 147.2ms\n",
            "video 1/1 (frame 53/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 6 cars, 127.2ms\n",
            "video 1/1 (frame 54/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 6 cars, 119.5ms\n",
            "video 1/1 (frame 55/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 116.8ms\n",
            "video 1/1 (frame 56/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 121.1ms\n",
            "video 1/1 (frame 57/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 121.1ms\n",
            "video 1/1 (frame 58/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 125.2ms\n",
            "video 1/1 (frame 59/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 122.6ms\n",
            "video 1/1 (frame 60/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 129.1ms\n",
            "video 1/1 (frame 61/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 119.1ms\n",
            "video 1/1 (frame 62/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 118.6ms\n",
            "video 1/1 (frame 63/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 116.3ms\n",
            "video 1/1 (frame 64/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 124.2ms\n",
            "video 1/1 (frame 65/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 135.8ms\n",
            "video 1/1 (frame 66/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 124.1ms\n",
            "video 1/1 (frame 67/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 120.3ms\n",
            "video 1/1 (frame 68/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 122.7ms\n",
            "video 1/1 (frame 69/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 132.3ms\n",
            "video 1/1 (frame 70/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 192.2ms\n",
            "video 1/1 (frame 71/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 208.2ms\n",
            "video 1/1 (frame 72/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 182.3ms\n",
            "video 1/1 (frame 73/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 182.7ms\n",
            "video 1/1 (frame 74/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 2 cars, 1 truck, 338.8ms\n",
            "video 1/1 (frame 75/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 1 truck, 427.6ms\n",
            "video 1/1 (frame 76/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 1 truck, 462.2ms\n",
            "video 1/1 (frame 77/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 637.9ms\n",
            "video 1/1 (frame 78/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 268.7ms\n",
            "video 1/1 (frame 79/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 119.2ms\n",
            "video 1/1 (frame 80/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 117.7ms\n",
            "video 1/1 (frame 81/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 7 cars, 122.3ms\n",
            "video 1/1 (frame 82/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 122.7ms\n",
            "video 1/1 (frame 83/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 118.0ms\n",
            "video 1/1 (frame 84/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 1 person, 4 cars, 118.5ms\n",
            "video 1/1 (frame 85/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 1 truck, 131.2ms\n",
            "video 1/1 (frame 86/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 1 truck, 124.1ms\n",
            "video 1/1 (frame 87/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 6 cars, 124.5ms\n",
            "video 1/1 (frame 88/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 6 cars, 122.2ms\n",
            "video 1/1 (frame 89/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 119.5ms\n",
            "video 1/1 (frame 90/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 121.0ms\n",
            "video 1/1 (frame 91/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 130.6ms\n",
            "video 1/1 (frame 92/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 118.2ms\n",
            "video 1/1 (frame 93/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 1 traffic light, 121.4ms\n",
            "video 1/1 (frame 94/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 6 cars, 1 traffic light, 127.0ms\n",
            "video 1/1 (frame 95/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 122.5ms\n",
            "video 1/1 (frame 96/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 6 cars, 123.9ms\n",
            "video 1/1 (frame 97/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 120.7ms\n",
            "video 1/1 (frame 98/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 135.9ms\n",
            "video 1/1 (frame 99/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 7 cars, 119.5ms\n",
            "video 1/1 (frame 100/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 8 cars, 1 truck, 1 traffic light, 124.4ms\n",
            "video 1/1 (frame 101/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 1 truck, 1 traffic light, 139.0ms\n",
            "video 1/1 (frame 102/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 1 truck, 1 traffic light, 125.2ms\n",
            "video 1/1 (frame 103/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 1 traffic light, 117.7ms\n",
            "video 1/1 (frame 104/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 7 cars, 1 truck, 119.8ms\n",
            "video 1/1 (frame 105/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 6 cars, 1 truck, 138.5ms\n",
            "video 1/1 (frame 106/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 121.1ms\n",
            "video 1/1 (frame 107/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 122.7ms\n",
            "video 1/1 (frame 108/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 126.4ms\n",
            "video 1/1 (frame 109/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 119.4ms\n",
            "video 1/1 (frame 110/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 121.0ms\n",
            "video 1/1 (frame 111/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 1 truck, 135.3ms\n",
            "video 1/1 (frame 112/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 1 traffic light, 131.3ms\n",
            "video 1/1 (frame 113/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 8 cars, 1 truck, 122.7ms\n",
            "video 1/1 (frame 114/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 124.9ms\n",
            "video 1/1 (frame 115/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 121.0ms\n",
            "video 1/1 (frame 116/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 120.1ms\n",
            "video 1/1 (frame 117/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 7 cars, 1 traffic light, 119.1ms\n",
            "video 1/1 (frame 118/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 1 truck, 141.0ms\n",
            "video 1/1 (frame 119/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 (no detections), 122.5ms\n",
            "video 1/1 (frame 120/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 1 truck, 1 traffic light, 123.0ms\n",
            "video 1/1 (frame 121/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 6 cars, 1 truck, 121.2ms\n",
            "video 1/1 (frame 122/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 1 truck, 120.5ms\n",
            "video 1/1 (frame 123/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 6 cars, 118.5ms\n",
            "video 1/1 (frame 124/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 6 cars, 1 traffic light, 118.8ms\n",
            "video 1/1 (frame 125/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 6 cars, 1 traffic light, 133.9ms\n",
            "video 1/1 (frame 126/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 6 cars, 155.0ms\n",
            "video 1/1 (frame 127/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 125.5ms\n",
            "video 1/1 (frame 128/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 1 traffic light, 122.0ms\n",
            "video 1/1 (frame 129/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 122.0ms\n",
            "video 1/1 (frame 130/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 119.8ms\n",
            "video 1/1 (frame 131/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 6 cars, 137.1ms\n",
            "video 1/1 (frame 132/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 126.6ms\n",
            "video 1/1 (frame 133/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 6 cars, 1 traffic light, 123.1ms\n",
            "video 1/1 (frame 134/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 121.3ms\n",
            "video 1/1 (frame 135/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 6 cars, 1 truck, 120.2ms\n",
            "video 1/1 (frame 136/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 1 truck, 120.8ms\n",
            "video 1/1 (frame 137/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 6 cars, 119.9ms\n",
            "video 1/1 (frame 138/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 6 cars, 133.2ms\n",
            "video 1/1 (frame 139/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 139.1ms\n",
            "video 1/1 (frame 140/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 123.2ms\n",
            "video 1/1 (frame 141/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 6 cars, 117.6ms\n",
            "video 1/1 (frame 142/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 7 cars, 176.3ms\n",
            "video 1/1 (frame 143/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 6 cars, 183.2ms\n",
            "video 1/1 (frame 144/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 198.4ms\n",
            "video 1/1 (frame 145/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 189.3ms\n",
            "video 1/1 (frame 146/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 181.6ms\n",
            "video 1/1 (frame 147/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 183.3ms\n",
            "video 1/1 (frame 148/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 1 traffic light, 221.5ms\n",
            "video 1/1 (frame 149/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 6 cars, 180.3ms\n",
            "video 1/1 (frame 150/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 7 cars, 183.2ms\n",
            "video 1/1 (frame 151/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 6 cars, 183.9ms\n",
            "video 1/1 (frame 152/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 6 cars, 189.8ms\n",
            "video 1/1 (frame 153/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 6 cars, 210.0ms\n",
            "video 1/1 (frame 154/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 180.4ms\n",
            "video 1/1 (frame 155/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 182.7ms\n",
            "video 1/1 (frame 156/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 116.8ms\n",
            "video 1/1 (frame 157/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 1 truck, 123.2ms\n",
            "video 1/1 (frame 158/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 147.5ms\n",
            "video 1/1 (frame 159/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 2 cars, 120.2ms\n",
            "video 1/1 (frame 160/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 119.6ms\n",
            "video 1/1 (frame 161/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 2 cars, 118.1ms\n",
            "video 1/1 (frame 162/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 1 traffic light, 120.8ms\n",
            "video 1/1 (frame 163/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 125.1ms\n",
            "video 1/1 (frame 164/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 1 traffic light, 122.7ms\n",
            "video 1/1 (frame 165/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 135.6ms\n",
            "video 1/1 (frame 166/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 6 cars, 1 traffic light, 123.8ms\n",
            "video 1/1 (frame 167/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 8 cars, 1 traffic light, 118.9ms\n",
            "video 1/1 (frame 168/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 6 cars, 120.7ms\n",
            "video 1/1 (frame 169/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 1 motorcycle, 119.9ms\n",
            "video 1/1 (frame 170/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 1 bus, 123.5ms\n",
            "video 1/1 (frame 171/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 125.7ms\n",
            "video 1/1 (frame 172/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 6 cars, 1 bus, 132.9ms\n",
            "video 1/1 (frame 173/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 7 cars, 1 bus, 117.5ms\n",
            "video 1/1 (frame 174/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 121.6ms\n",
            "video 1/1 (frame 175/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 118.9ms\n",
            "video 1/1 (frame 176/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 1 bus, 124.6ms\n",
            "video 1/1 (frame 177/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 131.0ms\n",
            "video 1/1 (frame 178/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 135.4ms\n",
            "video 1/1 (frame 179/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 117.9ms\n",
            "video 1/1 (frame 180/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 1 bus, 119.2ms\n",
            "video 1/1 (frame 181/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 1 bus, 120.2ms\n",
            "video 1/1 (frame 182/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 1 bus, 116.6ms\n",
            "video 1/1 (frame 183/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 1 bus, 122.3ms\n",
            "video 1/1 (frame 184/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 6 cars, 1 bus, 122.3ms\n",
            "video 1/1 (frame 185/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 136.4ms\n",
            "video 1/1 (frame 186/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 1 truck, 123.4ms\n",
            "video 1/1 (frame 187/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 1 truck, 120.6ms\n",
            "video 1/1 (frame 188/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 1 bus, 2 trucks, 122.6ms\n",
            "video 1/1 (frame 189/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 1 bus, 1 truck, 120.2ms\n",
            "video 1/1 (frame 190/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 1 bus, 2 trucks, 126.1ms\n",
            "video 1/1 (frame 191/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 1 bus, 2 trucks, 122.8ms\n",
            "video 1/1 (frame 192/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 1 bus, 1 truck, 137.4ms\n",
            "video 1/1 (frame 193/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 6 cars, 1 bus, 1 truck, 118.2ms\n",
            "video 1/1 (frame 194/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 1 bus, 1 truck, 121.2ms\n",
            "video 1/1 (frame 195/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 1 bus, 1 truck, 118.2ms\n",
            "video 1/1 (frame 196/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 6 cars, 1 bus, 1 truck, 124.4ms\n",
            "video 1/1 (frame 197/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 1 bus, 123.6ms\n",
            "video 1/1 (frame 198/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 118.1ms\n",
            "video 1/1 (frame 199/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 6 cars, 1 bus, 132.0ms\n",
            "video 1/1 (frame 200/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 6 cars, 1 truck, 122.6ms\n",
            "video 1/1 (frame 201/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 1 truck, 119.6ms\n",
            "video 1/1 (frame 202/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 1 truck, 132.4ms\n",
            "video 1/1 (frame 203/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 123.9ms\n",
            "video 1/1 (frame 204/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 132.8ms\n",
            "video 1/1 (frame 205/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 130.8ms\n",
            "video 1/1 (frame 206/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 6 cars, 119.5ms\n",
            "video 1/1 (frame 207/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 121.4ms\n",
            "video 1/1 (frame 208/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 1 truck, 119.3ms\n",
            "video 1/1 (frame 209/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 118.0ms\n",
            "video 1/1 (frame 210/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 133.5ms\n",
            "video 1/1 (frame 211/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 121.9ms\n",
            "video 1/1 (frame 212/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 133.5ms\n",
            "video 1/1 (frame 213/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 119.7ms\n",
            "video 1/1 (frame 214/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 7 cars, 120.3ms\n",
            "video 1/1 (frame 215/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 121.9ms\n",
            "video 1/1 (frame 216/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 1 bus, 126.1ms\n",
            "video 1/1 (frame 217/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 1 traffic light, 126.8ms\n",
            "video 1/1 (frame 218/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 120.1ms\n",
            "video 1/1 (frame 219/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 139.1ms\n",
            "video 1/1 (frame 220/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 6 cars, 139.6ms\n",
            "video 1/1 (frame 221/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 7 cars, 193.5ms\n",
            "video 1/1 (frame 222/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 197.6ms\n",
            "video 1/1 (frame 223/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 183.6ms\n",
            "video 1/1 (frame 224/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 1 bus, 199.2ms\n",
            "video 1/1 (frame 225/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 1 bus, 183.5ms\n",
            "video 1/1 (frame 226/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 1 truck, 184.9ms\n",
            "video 1/1 (frame 227/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 1 bus, 183.7ms\n",
            "video 1/1 (frame 228/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 2 buss, 190.2ms\n",
            "video 1/1 (frame 229/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 1 bus, 1 truck, 177.8ms\n",
            "video 1/1 (frame 230/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 2 buss, 189.8ms\n",
            "video 1/1 (frame 231/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 2 buss, 190.1ms\n",
            "video 1/1 (frame 232/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 2 buss, 1 truck, 190.5ms\n",
            "video 1/1 (frame 233/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 2 buss, 2 trucks, 210.4ms\n",
            "video 1/1 (frame 234/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 6 cars, 170.2ms\n",
            "video 1/1 (frame 235/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 1 bus, 129.9ms\n",
            "video 1/1 (frame 236/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 6 cars, 119.1ms\n",
            "video 1/1 (frame 237/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 120.3ms\n",
            "video 1/1 (frame 238/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 120.9ms\n",
            "video 1/1 (frame 239/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 1 bus, 2 trucks, 134.8ms\n",
            "video 1/1 (frame 240/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 1 truck, 125.8ms\n",
            "video 1/1 (frame 241/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 6 cars, 1 truck, 126.0ms\n",
            "video 1/1 (frame 242/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 2 trucks, 119.9ms\n",
            "video 1/1 (frame 243/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 1 truck, 117.7ms\n",
            "video 1/1 (frame 244/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 2 trucks, 117.7ms\n",
            "video 1/1 (frame 245/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 2 trucks, 137.8ms\n",
            "video 1/1 (frame 246/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 125.3ms\n",
            "video 1/1 (frame 247/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 1 bus, 2 trucks, 124.0ms\n",
            "video 1/1 (frame 248/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 121.7ms\n",
            "video 1/1 (frame 249/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 1 truck, 118.3ms\n",
            "video 1/1 (frame 250/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 1 truck, 119.0ms\n",
            "video 1/1 (frame 251/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 1 bus, 1 truck, 117.7ms\n",
            "video 1/1 (frame 252/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 1 bus, 140.0ms\n",
            "video 1/1 (frame 253/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 1 truck, 121.8ms\n",
            "video 1/1 (frame 254/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 6 cars, 125.2ms\n",
            "video 1/1 (frame 255/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 122.5ms\n",
            "video 1/1 (frame 256/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 1 truck, 120.5ms\n",
            "video 1/1 (frame 257/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 6 cars, 1 truck, 119.1ms\n",
            "video 1/1 (frame 258/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 6 cars, 1 truck, 137.4ms\n",
            "video 1/1 (frame 259/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 115.6ms\n",
            "video 1/1 (frame 260/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 1 bus, 124.1ms\n",
            "video 1/1 (frame 261/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 1 bus, 119.2ms\n",
            "video 1/1 (frame 262/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 6 cars, 120.7ms\n",
            "video 1/1 (frame 263/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 116.5ms\n",
            "video 1/1 (frame 264/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 1 truck, 120.0ms\n",
            "video 1/1 (frame 265/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 1 truck, 135.8ms\n",
            "video 1/1 (frame 266/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 6 cars, 1 truck, 127.0ms\n",
            "video 1/1 (frame 267/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 1 truck, 119.2ms\n",
            "video 1/1 (frame 268/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 6 cars, 1 truck, 115.9ms\n",
            "video 1/1 (frame 269/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 6 cars, 1 truck, 126.2ms\n",
            "video 1/1 (frame 270/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 7 cars, 117.2ms\n",
            "video 1/1 (frame 271/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 7 cars, 125.5ms\n",
            "video 1/1 (frame 272/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 9 cars, 1 truck, 135.3ms\n",
            "video 1/1 (frame 273/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 6 cars, 126.3ms\n",
            "video 1/1 (frame 274/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 7 cars, 115.7ms\n",
            "video 1/1 (frame 275/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 120.9ms\n",
            "video 1/1 (frame 276/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 6 cars, 1 bus, 119.9ms\n",
            "video 1/1 (frame 277/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 1 person, 6 cars, 125.7ms\n",
            "video 1/1 (frame 278/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 2 persons, 6 cars, 128.8ms\n",
            "video 1/1 (frame 279/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 2 persons, 6 cars, 1 bus, 132.0ms\n",
            "video 1/1 (frame 280/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 1 bus, 121.1ms\n",
            "video 1/1 (frame 281/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 6 cars, 1 bus, 121.1ms\n",
            "video 1/1 (frame 282/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 8 cars, 1 bus, 122.9ms\n",
            "video 1/1 (frame 283/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 7 cars, 1 bus, 118.4ms\n",
            "video 1/1 (frame 284/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 6 cars, 1 bus, 124.8ms\n",
            "video 1/1 (frame 285/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 1 truck, 141.0ms\n",
            "video 1/1 (frame 286/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 119.4ms\n",
            "video 1/1 (frame 287/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 7 cars, 1 bus, 1 truck, 120.0ms\n",
            "video 1/1 (frame 288/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 8 cars, 126.3ms\n",
            "video 1/1 (frame 289/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 6 cars, 119.7ms\n",
            "video 1/1 (frame 290/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 2 cars, 1 truck, 121.5ms\n",
            "video 1/1 (frame 291/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 4 cars, 122.1ms\n",
            "video 1/1 (frame 292/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 137.2ms\n",
            "video 1/1 (frame 293/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 7 cars, 118.7ms\n",
            "video 1/1 (frame 294/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 6 cars, 116.4ms\n",
            "video 1/1 (frame 295/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 7 cars, 120.2ms\n",
            "video 1/1 (frame 296/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 6 cars, 119.0ms\n",
            "video 1/1 (frame 297/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 9 cars, 120.7ms\n",
            "video 1/1 (frame 298/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 11 cars, 169.5ms\n",
            "video 1/1 (frame 299/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 6 cars, 175.2ms\n",
            "video 1/1 (frame 300/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 8 cars, 190.3ms\n",
            "video 1/1 (frame 301/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 5 cars, 178.2ms\n",
            "video 1/1 (frame 302/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 3 cars, 197.0ms\n",
            "video 1/1 (frame 303/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 9 cars, 206.2ms\n",
            "video 1/1 (frame 304/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 7 cars, 181.5ms\n",
            "video 1/1 (frame 305/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 6 cars, 176.9ms\n",
            "video 1/1 (frame 306/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 8 cars, 1 bus, 183.1ms\n",
            "video 1/1 (frame 307/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 8 cars, 197.6ms\n",
            "video 1/1 (frame 308/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 7 cars, 2 buss, 183.5ms\n",
            "video 1/1 (frame 309/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 6 cars, 1 bus, 1 boat, 184.2ms\n",
            "video 1/1 (frame 310/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 9 cars, 182.4ms\n",
            "video 1/1 (frame 311/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 8 cars, 1 bus, 1 truck, 190.6ms\n",
            "video 1/1 (frame 312/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 10 cars, 2 buss, 179.5ms\n",
            "video 1/1 (frame 313/313) /content/KakaoTalk_20250717_091729282.mp4: 288x640 8 cars, 2 buss, 114.4ms\n",
            "Speed: 3.9ms preprocess, 139.7ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "Results saved to \u001b[1mruns/detect/train32\u001b[0m\n",
            "312 labels saved to runs/detect/train32/labels\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics yt-dlp opencv-python\n",
        "\n",
        "import cv2\n",
        "import os\n",
        "from ultralytics import YOLO\n",
        "from IPython.display import Video, display\n",
        "\n",
        "def download_and_analyze_youtube_video(youtube_url):\n",
        "    \"\"\"\n",
        "    YouTube 영상을 직접 다운로드하고 YOLO 분석\n",
        "    \"\"\"\n",
        "    print(\"⬇️ YouTube 영상 다운로드 중...\")\n",
        "\n",
        "    # YouTube 영상 다운로드 (yt-dlp 직접 명령어 사용)\n",
        "    os.system(f'yt-dlp -f \"bestvideo+bestaudio/best[height<=720]\" --merge-output-format mp4 \"{youtube_url}\"')\n",
        "\n",
        "    # 다운로드된 mp4 파일 찾기\n",
        "    video_file = None\n",
        "    for file in os.listdir(\"/content\"):\n",
        "        if file.endswith(\".mp4\"):\n",
        "            video_file = file\n",
        "            print(f\"📁 다운로드된 영상: {file}\")\n",
        "            break\n",
        "\n",
        "    if not video_file:\n",
        "        print(\"❌ 다운로드된 영상을 찾을 수 없습니다.\")\n",
        "        return None\n",
        "\n",
        "    # YOLO 모델 로드\n",
        "    print(\"🤖 YOLO 모델 로드 중...\")\n",
        "    model = YOLO(\"yolov8n.pt\")\n",
        "\n",
        "    # 비디오 캡처 설정\n",
        "    print(\"🎬 영상 분석 시작...\")\n",
        "    video_path = f\"/content/{video_file}\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # 비디오 정보 가져오기\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    duration = total_frames / fps\n",
        "\n",
        "    print(f\"📹 영상 정보:\")\n",
        "    print(f\"   해상도: {width}x{height}\")\n",
        "    print(f\"   FPS: {fps:.1f}\")\n",
        "    print(f\"   총 프레임: {total_frames}\")\n",
        "    print(f\"   길이: {duration:.1f}초\")\n",
        "\n",
        "    # 결과 저장 설정\n",
        "    output_path = \"result_video_youtube.mp4\"\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    # 분석 통계\n",
        "    frame_count = 0\n",
        "    detection_count = 0\n",
        "    frames_with_objects = 0\n",
        "\n",
        "    # 프레임별로 추론 및 저장\n",
        "    print(\"🔍 YOLO 분석 진행 중...\")\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # YOLOv8 추론\n",
        "        results = model(frame, verbose=False, conf=0.5)\n",
        "\n",
        "        # 탐지된 객체 수 계산\n",
        "        if len(results) > 0 and results[0].boxes is not None:\n",
        "            num_detections = len(results[0].boxes)\n",
        "            detection_count += num_detections\n",
        "            if num_detections > 0:\n",
        "                frames_with_objects += 1\n",
        "\n",
        "        # 결과 이미지 (bounding box 포함)\n",
        "        annotated_frame = results[0].plot()\n",
        "        out.write(annotated_frame)\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "        # 진행률 표시 (10% 단위)\n",
        "        if frame_count % (total_frames // 10) == 0:\n",
        "            progress = (frame_count / total_frames) * 100\n",
        "            print(f\"진행률: {progress:.0f}%\")\n",
        "\n",
        "    # 리소스 해제\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "    # 분석 결과 통계\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"📊 분석 완료! 결과 통계\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"총 처리 프레임: {frame_count}\")\n",
        "    print(f\"총 탐지된 객체: {detection_count}\")\n",
        "    print(f\"객체가 탐지된 프레임: {frames_with_objects}\")\n",
        "    if frame_count > 0:\n",
        "        print(f\"프레임당 평균 탐지 수: {detection_count/frame_count:.2f}\")\n",
        "        print(f\"객체 탐지율: {(frames_with_objects/frame_count)*100:.1f}%\")\n",
        "\n",
        "    print(f\"\\n✅ 결과 영상 저장: {output_path}\")\n",
        "\n",
        "    # 결과 영상 표시\n",
        "    print(\"🎥 결과 영상 재생:\")\n",
        "    return output_path\n",
        "\n",
        "def analyze_local_video(video_path):\n",
        "    \"\"\"\n",
        "    로컬 비디오 파일 YOLO 분석 (업로드된 파일용)\n",
        "    \"\"\"\n",
        "    if not os.path.exists(video_path):\n",
        "        print(f\"❌ 파일을 찾을 수 없습니다: {video_path}\")\n",
        "        return None\n",
        "\n",
        "    # YOLO 모델 로드\n",
        "    print(\"🤖 YOLO 모델 로드 중...\")\n",
        "    model = YOLO(\"yolov8n.pt\")\n",
        "\n",
        "    # 비디오 캡처 설정\n",
        "    print(\"🎬 영상 분석 시작...\")\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # 비디오 정보 가져오기\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    duration = total_frames / fps if fps > 0 else 0\n",
        "\n",
        "    print(f\"📹 영상 정보:\")\n",
        "    print(f\"   해상도: {width}x{height}\")\n",
        "    print(f\"   FPS: {fps:.1f}\")\n",
        "    print(f\"   총 프레임: {total_frames}\")\n",
        "    print(f\"   길이: {duration:.1f}초\")\n",
        "\n",
        "    # 결과 저장 설정\n",
        "    output_path = \"result_video_local.mp4\"\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    # 분석 통계\n",
        "    frame_count = 0\n",
        "    detection_count = 0\n",
        "    frames_with_objects = 0\n",
        "\n",
        "    # 프레임별로 추론 및 저장\n",
        "    print(\"🔍 YOLO 분석 진행 중...\")\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # YOLOv8 추론\n",
        "        results = model(frame, verbose=False, conf=0.5)\n",
        "\n",
        "        # 탐지된 객체 수 계산\n",
        "        if len(results) > 0 and results[0].boxes is not None:\n",
        "            num_detections = len(results[0].boxes)\n",
        "            detection_count += num_detections\n",
        "            if num_detections > 0:\n",
        "                frames_with_objects += 1\n",
        "\n",
        "        # 결과 이미지 (bounding box 포함)\n",
        "        annotated_frame = results[0].plot()\n",
        "        out.write(annotated_frame)\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "        # 진행률 표시 (10% 단위)\n",
        "        if total_frames > 0 and frame_count % max(1, total_frames // 10) == 0:\n",
        "            progress = (frame_count / total_frames) * 100\n",
        "            print(f\"진행률: {progress:.0f}%\")\n",
        "\n",
        "    # 리소스 해제\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "    # 분석 결과 통계\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"📊 분석 완료! 결과 통계\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"총 처리 프레임: {frame_count}\")\n",
        "    print(f\"총 탐지된 객체: {detection_count}\")\n",
        "    print(f\"객체가 탐지된 프레임: {frames_with_objects}\")\n",
        "    if frame_count > 0:\n",
        "        print(f\"프레임당 평균 탐지 수: {detection_count/frame_count:.2f}\")\n",
        "        print(f\"객체 탐지율: {(frames_with_objects/frame_count)*100:.1f}%\")\n",
        "\n",
        "    print(f\"\\n✅ 결과 영상 저장: {output_path}\")\n",
        "\n",
        "    return output_path\n",
        "\n",
        "# 메인 실행 부분\n",
        "print(\"🎬 YouTube YOLO 분석기 (간단 버전)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "mode = input(\"\\n선택하세요:\\n1. YouTube URL 입력\\n2. 파일 업로드\\n\\n선택 (1 또는 2): \")\n",
        "\n",
        "if mode == \"1\":\n",
        "    # YouTube URL 입력\n",
        "    youtube_url = input(\"\\nYouTube URL을 입력하세요: \")\n",
        "\n",
        "    result_path = download_and_analyze_youtube_video(youtube_url)\n",
        "\n",
        "    if result_path and os.path.exists(result_path):\n",
        "        # 결과 영상 표시\n",
        "        display(Video(result_path, embed=True, width=800))\n",
        "    else:\n",
        "        print(\"❌ 분석 실패\")\n",
        "\n",
        "elif mode == \"2\":\n",
        "    # 파일 업로드\n",
        "    print(\"\\n📁 비디오 파일을 업로드하세요:\")\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        uploaded = files.upload()\n",
        "\n",
        "        if uploaded:\n",
        "            video_file = list(uploaded.keys())[0]\n",
        "            print(f\"📹 업로드된 파일: {video_file}\")\n",
        "\n",
        "            result_path = analyze_local_video(video_file)\n",
        "\n",
        "            if result_path and os.path.exists(result_path):\n",
        "                # 결과 영상 표시\n",
        "                display(Video(result_path, embed=True, width=800))\n",
        "            else:\n",
        "                print(\"❌ 분석 실패\")\n",
        "        else:\n",
        "            print(\"❌ 파일이 업로드되지 않았습니다.\")\n",
        "\n",
        "    except ImportError:\n",
        "        # Colab이 아닌 환경\n",
        "        video_path = input(\"비디오 파일 경로를 입력하세요: \")\n",
        "        result_path = analyze_local_video(video_path)\n",
        "\n",
        "        if result_path and os.path.exists(result_path):\n",
        "            print(f\"✅ 결과 영상: {result_path}\")\n",
        "        else:\n",
        "            print(\"❌ 분석 실패\")\n",
        "\n",
        "else:\n",
        "    print(\"❌ 잘못된 선택입니다.\")\n",
        "\n",
        "print(\"\\n🎉 작업 완료!\")"
      ],
      "metadata": {
        "id": "QF6aTicxOg1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def play_and_save_youtube_with_yolo(youtube_url, skip_frames=5, output_filename=\"output_with_yolo.mp4\", max_duration=10):\n",
        "    \"\"\"\n",
        "    max_duration: 최대 처리 시간(초)\n",
        "    \"\"\"\n",
        "    ydl_opts = {\n",
        "        'format': 'mp4/best[height<=480]',\n",
        "        'outtmpl': tempfile.gettempdir() + '/temp_video.%(ext)s',\n",
        "        'quiet': True,\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "            print(\"⬇️ 유튜브 영상 다운로드 중...\")\n",
        "            info = ydl.extract_info(youtube_url, download=True)\n",
        "            video_path = ydl.prepare_filename(info)\n",
        "\n",
        "            print(f\"🎥 영상 제목: {info['title']}\")\n",
        "\n",
        "            cap = cv2.VideoCapture(video_path)\n",
        "            fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "            frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "            frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "            max_frames = int(fps * max_duration)\n",
        "            print(f\"⏱ 최대 {max_duration}초 (약 {max_frames} 프레임) 처리\")\n",
        "\n",
        "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "            out = cv2.VideoWriter(output_filename, fourcc, fps / skip_frames, (frame_width, frame_height))\n",
        "\n",
        "            frame_num = 0\n",
        "            processed_frames = 0\n",
        "\n",
        "            while True:\n",
        "                ret, frame = cap.read()\n",
        "                if not ret:\n",
        "                    break\n",
        "                if frame_num >= max_frames:\n",
        "                    break\n",
        "\n",
        "                if frame_num % skip_frames == 0:\n",
        "                    results = model.predict(frame, imgsz=640, verbose=False)[0]\n",
        "                    annotated_frame = results.plot()\n",
        "                    out.write(annotated_frame)\n",
        "\n",
        "                    clear_output(wait=True)\n",
        "                    plt.figure(figsize=(10, 6))\n",
        "                    plt.imshow(cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB))\n",
        "                    plt.title(f\"YOLOv8 추론 프레임 ({frame_num})\")\n",
        "                    plt.axis('off')\n",
        "                    plt.show()\n",
        "\n",
        "                    time.sleep(0.1)\n",
        "\n",
        "                frame_num += 1\n",
        "\n",
        "            cap.release()\n",
        "            out.release()\n",
        "            os.remove(video_path)\n",
        "            print(f\"✅ 영상 추론 종료, 저장 파일: {output_filename}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ 오류 발생: {e}\")\n",
        "\n",
        "# 실행\n",
        "youtube_url = input(\"유튜브 영상 URL 입력: \")\n",
        "play_and_save_youtube_with_yolo(youtube_url, skip_frames=5, max_duration=10)\n"
      ],
      "metadata": {
        "id": "b_yYp0__wE8c"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Colab 시작하기",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}